const OpenAI = require('openai');
const axios = require('axios');

// AI ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî
let openai = null;
let ollama = null;
const apiKey = process.env.OPENAI_API_KEY;

// OpenAI Ï¥àÍ∏∞Ìôî
if (apiKey && apiKey.startsWith('sk-') && !apiKey.includes('demo') && !apiKey.includes('test')) {
  try {
    openai = new OpenAI({
      apiKey: apiKey
    });
    console.log('‚úÖ OpenAI client initialized with API key');
  } catch (error) {
    console.warn('‚ö†Ô∏è OpenAI initialization failed:', error.message);
    openai = null;
  }
} else if (apiKey) {
  console.warn('‚ö†Ô∏è Demo/test OpenAI API key detected. Will try Ollama first.');
} else {
  console.warn('‚ö†Ô∏è OpenAI API key not found. Will try Ollama first.');
}

// Ollama ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî Î∞è ÌÖåÏä§Ìä∏
async function initializeOllama() {
  try {
    const response = await axios.get('http://localhost:11434/api/tags');
    if (response.data && response.data.models) {
      const availableModels = response.data.models;
      console.log('ü¶ô Ollama available models:', availableModels.map(m => m.name));
      
      // llama3.1 Î™®Îç∏ ÌôïÏù∏
      const llama31Model = availableModels.find(m => m.name.includes('llama3.1'));
      if (llama31Model) {
        ollama = {
          baseURL: 'http://localhost:11434',
          model: llama31Model.name
        };
        console.log(`‚úÖ Ollama client initialized with model: ${llama31Model.name}`);
        return true;
      }
    }
  } catch (error) {
    console.warn('‚ö†Ô∏è Ollama not available:', error.message);
  }
  return false;
}

// Ollama Ï¥àÍ∏∞Ìôî ÏãúÎèÑ
initializeOllama();

// Ollama API Ìò∏Ï∂ú Ìï®Ïàò
async function callOllama(messages, systemPrompt = '') {
  if (!ollama) return null;
  
  try {
    // Ollama Î©îÏãúÏßÄ Ìè¨Îß∑ Î≥ÄÌôò
    const ollamaMessages = [];
    if (systemPrompt) {
      ollamaMessages.push({
        role: 'system',
        content: systemPrompt
      });
    }
    ollamaMessages.push(...messages);

    const response = await axios.post(`${ollama.baseURL}/api/chat`, {
      model: ollama.model,
      messages: ollamaMessages,
      stream: false,
      options: {
        temperature: 0.7,
        top_p: 0.9,
        max_tokens: 150
      }
    });

    if (response.data && response.data.message) {
      return response.data.message.content;
    }
  } catch (error) {
    console.warn('ü¶ô Ollama API error:', error.message);
  }
  return null;
}

const openaiService = {
  // Ï≤´ Î©îÏãúÏßÄ ÏÉùÏÑ±
  async generateFirstMessage(situation) {
    const systemPrompt = `You are an English conversation partner helping Korean learners practice real-life situations.
    Current situation: ${situation.title}
    Description: ${situation.description}
    Difficulty: ${situation.difficulty}
    
    Start the conversation naturally as if you are in this real situation.
    Keep it simple for ${situation.difficulty} level.
    Be encouraging and patient.`;

    // 1. Ollama ÏãúÎèÑ (Î¨¥Î£å, Î°úÏª¨)
    if (ollama) {
      const ollamaResponse = await callOllama([
        { role: "user", content: `Start the conversation for: ${situation.title}` }
      ], systemPrompt);
      
      if (ollamaResponse) {
        console.log('ü¶ô Using Ollama for first message');
        return ollamaResponse;
      }
    }

    // 2. OpenAI ÏãúÎèÑ (Ïú†Î£å)
    if (openai) {
      try {
        const completion = await openai.chat.completions.create({
          model: "gpt-3.5-turbo",
          messages: [
            { role: "system", content: systemPrompt },
            { role: "user", content: `Start the conversation for: ${situation.title}` }
          ],
          temperature: 0.7,
          max_tokens: 150
        });

        console.log('üîë Using OpenAI for first message');
        return completion.choices[0].message.content;
      } catch (error) {
        console.error('OpenAI API error:', error);
      }
    }

    // 3. ÏßÄÎä•Ìòï Ìè¥Î∞± ÏãúÏä§ÌÖú
    console.log('üí° Using intelligent fallback for first message');
    return this.getFallbackFirstMessage(situation);
  },

  // AI ÏùëÎãµ ÏÉùÏÑ±
  async generateResponse(situation, messages, currentUserMessage = '') {
    const systemPrompt = `You are an English conversation partner in a ${situation.title} situation.
    Difficulty level: ${situation.difficulty}
    Keep responses natural and appropriate for the situation.
    Be encouraging and help the learner practice.
    If they make mistakes, continue the conversation naturally without directly correcting them.`;

    // ÏµúÍ∑º 5Í∞ú Î©îÏãúÏßÄÎßå Ïª®ÌÖçÏä§Ìä∏Î°ú ÏÇ¨Ïö©
    const recentMessages = messages.slice(-5).map(msg => ({
      role: msg.role === 'assistant' ? 'assistant' : 'user',
      content: msg.content
    }));

    // 1. Ollama ÏãúÎèÑ (Î¨¥Î£å, Î°úÏª¨)
    if (ollama) {
      const ollamaResponse = await callOllama(recentMessages, systemPrompt);
      
      if (ollamaResponse) {
        console.log('ü¶ô Using Ollama for response generation');
        return {
          content: ollamaResponse,
          finishReason: 'stop'
        };
      }
    }

    // 2. OpenAI ÏãúÎèÑ (Ïú†Î£å)
    if (openai) {
      try {
        const completion = await openai.chat.completions.create({
          model: "gpt-3.5-turbo",
          messages: [
            { role: "system", content: systemPrompt },
            ...recentMessages
          ],
          temperature: 0.7,
          max_tokens: 150
        });

        console.log('üîë Using OpenAI for response generation');
        return {
          content: completion.choices[0].message.content,
          finishReason: completion.choices[0].finish_reason
        };
      } catch (error) {
        console.error('OpenAI API error:', error);
      }
    }

    // 3. ÏßÄÎä•Ìòï Ìè¥Î∞± ÏãúÏä§ÌÖú
    console.log('üí° Using intelligent fallback for response');
    return {
      content: this.getFallbackResponse(situation, currentUserMessage),
      finishReason: 'fallback'
    };
  },

  // ÌîºÎìúÎ∞± Î∂ÑÏÑù
  async analyzeFeedback(userMessage, situation) {
    if (!openai) {
      return {
        corrections: [],
        suggestions: ["Keep practicing! API not available."],
        score: 75,
        positives: ["Good effort!"]
      };
    }
    
    try {
      const systemPrompt = `You are an English teacher analyzing a student's response.
      Situation: ${situation.title}
      Difficulty: ${situation.difficulty}
      
      Analyze the grammar, vocabulary, and appropriateness.
      Provide constructive feedback in a supportive way.
      Return JSON format with corrections and suggestions.`;

      const completion = await openai.chat.completions.create({
        model: "gpt-3.5-turbo",
        messages: [
          { role: "system", content: systemPrompt },
          { role: "user", content: `Analyze this response: "${userMessage}"` }
        ],
        temperature: 0.3,
        max_tokens: 200,
        response_format: { type: "json_object" }
      });

      const feedback = JSON.parse(completion.choices[0].message.content);
      
      return {
        corrections: feedback.corrections || [],
        suggestions: feedback.suggestions || [],
        score: feedback.score || 70,
        positives: feedback.positives || []
      };
    } catch (error) {
      console.error('Feedback analysis error:', error);
      return {
        corrections: [],
        suggestions: ["Keep practicing! You're doing great."],
        score: 70,
        positives: ["Good effort!"]
      };
    }
  },

  // ÏÑ∏ÏÖò ÌèâÍ∞Ä
  async evaluateSession(messages, situation, feedbackHistory) {
    if (!openai) {
      return this.getDefaultEvaluation();
    }
    
    try {
      const userMessages = messages.filter(m => m.role === 'user');
      
      const systemPrompt = `Evaluate the overall performance in this English conversation practice.
      Situation: ${situation.title}
      Total exchanges: ${messages.length}
      
      Consider: grammar accuracy, vocabulary usage, conversation flow, appropriateness.
      Provide an overall score (0-100) and detailed feedback.
      Return JSON format.`;

      const completion = await openai.chat.completions.create({
        model: "gpt-3.5-turbo",
        messages: [
          { role: "system", content: systemPrompt },
          { 
            role: "user", 
            content: `Evaluate these responses: ${JSON.stringify(userMessages.map(m => m.content))}`
          }
        ],
        temperature: 0.3,
        max_tokens: 300,
        response_format: { type: "json_object" }
      });

      const evaluation = JSON.parse(completion.choices[0].message.content);
      
      return {
        score: evaluation.score || 75,
        strengths: evaluation.strengths || ["Good participation"],
        improvements: evaluation.improvements || ["Keep practicing"],
        grammar: evaluation.grammar || 70,
        vocabulary: evaluation.vocabulary || 70,
        fluency: evaluation.fluency || 70,
        appropriateness: evaluation.appropriateness || 80
      };
    } catch (error) {
      console.error('Session evaluation error:', error);
      return this.getDefaultEvaluation();
    }
  },

  // Ìè¥Î∞± Î©îÏãúÏßÄÎì§
  getFallbackFirstMessage(situation) {
    const fallbacks = {
      'daegu_taxi': "Hello! Where would you like to go today?",
      'daegu_food': "Welcome! How many people in your party?",
      'daegu_shopping': "Hello! What are you looking for today?",
      'coffee_order': "Hi! Welcome to our cafe. What can I get for you today?",
      'hospital_visit': "Hello, how can I help you today? What brings you in?",
      'meeting_intro': "Good morning! Nice to meet you. I'm glad we could arrange this meeting."
    };
    
    return fallbacks[situation.id] || "Hello! How can I help you today?";
  },

  getFallbackResponse(situation, userMessage = '') {
    // ÏÉÅÌô© ID Ï∂îÏ∂ú
    const situationId = typeof situation === 'string' ? situation : situation?.title || situation?.id || 'default';
    
    // ÏÇ¨Ïö©Ïûê Î©îÏãúÏßÄÎ•º ÏÜåÎ¨∏ÏûêÎ°ú Î≥ÄÌôòÌï¥ÏÑú ÌÇ§ÏõåÎìú Î∂ÑÏÑù
    const message = userMessage.toLowerCase();
    
    // ÌÇ§ÏõåÎìú Í∏∞Î∞ò ÏßÄÎä•Ìòï ÏùëÎãµ ÏãúÏä§ÌÖú
    return this.getSmartResponse(situationId, message);
  },

  // ÏßÄÎä•Ìòï ÌÇ§ÏõåÎìú Î∂ÑÏÑù ÏùëÎãµ ÏãúÏä§ÌÖú
  getSmartResponse(situationId, message) {
    // Í≥µÌÜµ ÌÇ§ÏõåÎìú ÏùëÎãµ (Î™®Îì† ÏÉÅÌô©ÏóêÏÑú ÏÇ¨Ïö©)
    const commonResponses = {
      greetings: {
        keywords: ['hi', 'hello', 'hey', 'good morning', 'good afternoon', 'good evening'],
        responses: {
          'daegu_taxi': "Hello! Where would you like to go today?",
          'daegu_food': "Welcome! What would you like to try today?",
          'daegu_shopping': "Hello! What are you looking for?",
          'coffee_order': "Hi! Welcome to our cafe. What can I get you?",
          'hospital_visit': "Hello! How can I help you today?",
          'meeting_intro': "Hello! Nice to meet you. Thanks for coming.",
          'default': "Hello! How can I help you today?"
        }
      },
      thanks: {
        keywords: ['thank', 'thanks', 'thank you'],
        responses: {
          'default': "You're welcome! Is there anything else I can help you with?"
        }
      },
      goodbye: {
        keywords: ['bye', 'goodbye', 'see you', 'have a good'],
        responses: {
          'default': "Goodbye! Have a wonderful day!"
        }
      },
      how_are_you: {
        keywords: ['how are you', 'how do you do', 'how have you been'],
        responses: {
          'default': "I'm doing great, thank you for asking! How about you?"
        }
      }
    };

    // ÏÉÅÌô©Î≥Ñ ÌäπÏ†ï ÌÇ§ÏõåÎìú ÏùëÎãµ
    const situationSpecificResponses = {
      'daegu_taxi': {
        location: {
          keywords: ['go to', 'take me', 'airport', 'station', 'hotel', 'downtown', 'suseong', 'dongdaegu'],
          responses: [
            "Sure, I can take you there. It will take about {time} minutes.",
            "That's a popular destination. Do you have the exact address?",
            "No problem! That's about {distance} away from here.",
            "Of course! I know that area well."
          ]
        },
        price: {
          keywords: ['how much', 'cost', 'price', 'fare', 'expensive'],
          responses: [
            "The fare will be around 10,000 to 15,000 won depending on traffic.",
            "It's usually about 12,000 won to that area.",
            "Don't worry, it's not too expensive. About 8,000 won."
          ]
        },
        time: {
          keywords: ['how long', 'time', 'minutes', 'quick', 'fast'],
          responses: [
            "It will take about 15-20 minutes depending on traffic.",
            "Usually about 10 minutes, but there might be some traffic.",
            "Pretty quick - just 5 minutes from here."
          ]
        }
      },
      'daegu_food': {
        recommendation: {
          keywords: ['recommend', 'suggest', 'popular', 'famous', 'best', 'good'],
          responses: [
            "I recommend our bulgogi! It's very popular with customers.",
            "Our bibimbap is excellent - it's a local specialty.",
            "You should try the galbi. It's one of our signature dishes.",
            "The kimchi jjigae is really good here."
          ]
        },
        spicy: {
          keywords: ['spicy', 'hot', 'mild', 'not spicy'],
          responses: [
            "How spicy would you like it? We can make it mild or very spicy.",
            "This dish is quite spicy. Would you like it less spicy?",
            "Don't worry, this one is not spicy at all.",
            "We can adjust the spice level for you."
          ]
        },
        menu: {
          keywords: ['menu', 'what do you have', 'options', 'dishes'],
          responses: [
            "We have Korean BBQ, bibimbap, bulgogi, and many soups.",
            "Our menu includes traditional Korean dishes and some fusion options.",
            "Would you like to see our menu? We have pictures too."
          ]
        }
      },
      'coffee_order': {
        drinks: {
          keywords: ['coffee', 'latte', 'americano', 'cappuccino', 'tea', 'drink'],
          responses: [
            "Great choice! Would you like that hot or iced?",
            "What size would you prefer - small, medium, or large?",
            "That's one of our most popular drinks!",
            "Would you like any extra shots or syrup with that?"
          ]
        },
        size: {
          keywords: ['size', 'small', 'medium', 'large', 'grande', 'venti'],
          responses: [
            "Perfect! And would you like that hot or iced?",
            "Good choice! Anything else I can add for you?",
            "Coming right up! Will that be for here or to go?"
          ]
        },
        food: {
          keywords: ['sandwich', 'cake', 'pastry', 'cookie', 'muffin', 'food'],
          responses: [
            "We have fresh sandwiches and pastries today!",
            "Our chocolate cake is really popular. Would you like to try it?",
            "All our pastries are made fresh daily."
          ]
        }
      }
    };

    // 1. Í≥µÌÜµ ÌÇ§ÏõåÎìú Ï≤¥ÌÅ¨
    for (const [category, data] of Object.entries(commonResponses)) {
      if (data.keywords.some(keyword => message.includes(keyword))) {
        const response = data.responses[situationId] || data.responses['default'];
        return response;
      }
    }

    // 2. ÏÉÅÌô©Î≥Ñ ÌÇ§ÏõåÎìú Ï≤¥ÌÅ¨
    if (situationSpecificResponses[situationId]) {
      for (const [category, data] of Object.entries(situationSpecificResponses[situationId])) {
        if (data.keywords.some(keyword => message.includes(keyword))) {
          const responses = data.responses;
          const randomResponse = responses[Math.floor(Math.random() * responses.length)];
          // Í∞ÑÎã®Ìïú Î≥ÄÏàò ÏπòÌôò
          return randomResponse
            .replace('{time}', Math.floor(Math.random() * 20) + 5)
            .replace('{distance}', Math.floor(Math.random() * 10) + 2 + 'km');
        }
      }
    }

    // 3. Í∏∞Î≥∏ ÏÉÅÌô©Î≥Ñ ÏùëÎãµ
    const defaultSituationResponses = {
      'daegu_taxi': [
        "Where would you like to go?",
        "Which area in Daegu are you heading to?",
        "Do you have a specific destination in mind?"
      ],
      'daegu_food': [
        "What kind of food are you in the mood for?",
        "Are you familiar with Korean cuisine?",
        "Would you like me to recommend something?"
      ],
      'daegu_shopping': [
        "What are you looking for today?",
        "Are you looking for anything specific?",
        "Let me know if you need any help finding something."
      ],
      'coffee_order': [
        "What can I get started for you?",
        "What would you like to try today?",
        "How can I help you today?"
      ],
      'hospital_visit': [
        "How are you feeling today?",
        "What brings you in today?",
        "How can I assist you?"
      ],
      'meeting_intro': [
        "Nice to meet you!",
        "Thank you for meeting with me.",
        "I'm looking forward to our discussion."
      ]
    };

    const responses = defaultSituationResponses[situationId] || [
      "I see. Can you tell me more about that?",
      "That's interesting. How can I help you?",
      "Could you please tell me more details?"
    ];

    return responses[Math.floor(Math.random() * responses.length)];
  },

  getDefaultEvaluation() {
    return {
      score: 75,
      strengths: [
        "Good effort in completing the conversation",
        "Showed willingness to communicate"
      ],
      improvements: [
        "Practice more common phrases",
        "Work on grammar structures"
      ],
      grammar: 70,
      vocabulary: 70,
      fluency: 70,
      appropriateness: 80
    };
  }
};

module.exports = openaiService;